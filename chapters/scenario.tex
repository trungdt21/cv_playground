\section{Regularizing Knowledge distillation}
As one of the most beneficial characteristic of KD, any student can learn from any teacher disregarding the structural difference. But it is emperically proved that well-trained large DNN doesn't often make good teachers due to the mismatched capacity, which makes the student unable to mimic it \cite{complexitygap}. 

To tackle this problem, multiple work shares the same idea of regularizing the teacher model \cite{complexitygap, labelsmoothingnoise}. In \cite{complexitygap}, Cho et al. proposed a new process ESKD (Early-stopped knowledge distillation) after emprically prove sequential knowledge distillation is also not that efficient since it can only outperform one model training from scratch but not ensemble of those. They argued that the found solution space of the teacher is not accessible from the student, which means to find a teacher whose discovered solution should be discoverable by the student. Based on another works, the author assumed early stopping allows large model to behave as a small network while still having better search space than smaller ones. Different from Cho et al., Lukasik et al. \cite{labelsmoothingnoise} investigates the denoising effect of label smoothing on noisy data then applies it on the distillation process in order to test its effectiveness. The ending result is that applying label smoothing on the teacher significantly enhances over vanilla distilaltion, while applying the same on the student has mixed results.

Having the same idea, Li et al. \cite{teacherfree} investigates and compares KD with label smoothing regularizer and later on proposed a novel Teacher-free Knowledge distillation (TfKD) framework. It started with the observation that using either poorly trained teacher to distil student or student to teach teacher model still can improve and enhanced the guided models, which suggests the author to consider KD as a regularization term (strongly related to label smoothing regularizer). Having that in mind, the paper consider replacing the class distribution predictions of teacher model with a simppler one, implemented in the TfKD framework. This framework is particularly useful in circumstances where a more efficient teacher model is inaccessible or where only minimal computing resources are available. There are two methods of distilling in TfKD framework: 1) self-training distillation and 2) Combine KD with Label Smoothing Regularizer to create a 100\% accuracy teacher. Both of the methods are very simple yet effective and also emperically proved to be performant.